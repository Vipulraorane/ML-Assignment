{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cdc88936",
      "metadata": {
        "id": "cdc88936"
      },
      "source": [
        "### Question 1 : **What is Information Gain, and how is it used in Decision Trees?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe24fdda",
      "metadata": {
        "id": "fe24fdda"
      },
      "source": [
        "\n",
        "Information Gain measures the reduction in entropy (uncertainty) about the target label after splitting a dataset on a feature.  \n",
        "Formally, Information Gain for split S on feature A is:  \n",
        "**IG(S, A) = Entropy(S) - Σ (|S_v|/|S|) * Entropy(S_v)** where S_v are subsets after splitting by A.  \n",
        "Decision trees (like ID3/C4.5) use Information Gain to choose the best feature at each node — the feature yielding the largest IG is chosen because it reduces class impurity the most, producing purer child nodes and a more informative split.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b6fedd4",
      "metadata": {
        "id": "3b6fedd4"
      },
      "source": [
        "### Question 2: **What is the difference between Gini Impurity and Entropy?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dade16c",
      "metadata": {
        "id": "5dade16c"
      },
      "source": [
        "\n",
        "- **Entropy (Information Gain)**:  \n",
        "  - Formula: \\(H(S) = -\\sum_{i} p_i \\log_2 p_i\\).  \n",
        "  - Measures the amount of information (uncertainty). Sensitive to changes in class probability distribution; used by ID3/C4.5.  \n",
        "- **Gini Impurity**:  \n",
        "  - Formula: \\(G(S) = 1 - \\sum_{i} p_i^2\\).  \n",
        "  - Measures probability of misclassification when randomly labeling according to class distribution. Used by CART.  \n",
        "**Practical differences:**  \n",
        "- Both rank splits similarly in many cases; Gini is slightly faster to compute (no logarithm) and often preferred in CART implementations.  \n",
        "- Entropy is theoretically grounded in information theory and can be slightly more sensitive to class distribution changes.  \n",
        "- Choice often doesn't change final tree drastically; prefer whichever is implemented or benchmarked for your dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "197a06ea",
      "metadata": {
        "id": "197a06ea"
      },
      "source": [
        "### Question 3: **What is Pre-Pruning in Decision Trees?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "287976a1",
      "metadata": {
        "id": "287976a1"
      },
      "source": [
        "\n",
        "Pre-pruning (early stopping) halts tree growth early to prevent overfitting. Instead of fully growing the tree and pruning later, pre-pruning sets stopping rules while building the tree: e.g., minimum samples required to split, maximum depth, minimum impurity decrease, or minimum samples per leaf. These constraints prevent overly specific branches, improving generalization and reducing tree complexity, at the expense of possibly underfitting if too strict.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9efa7abd",
      "metadata": {
        "id": "9efa7abd"
      },
      "source": [
        "### Question 4: **Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "05290981",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05290981",
        "outputId": "91856389-be8a-4115-bfb1-14289ee89742"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature importances (Decision Tree, criterion='gini'):\n",
            "petal length (cm)    0.899746\n",
            "petal width (cm)     0.082378\n",
            "sepal width (cm)     0.017876\n",
            "sepal length (cm)    0.000000\n",
            "dtype: float64\n",
            "\n",
            "Train accuracy: 1.0\n",
            "Test accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Q4: Decision Tree Classifier using Gini and printing feature importances\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "feature_names = load_iris().feature_names\n",
        "dfX = pd.DataFrame(X, columns=feature_names)\n",
        "X_train, X_test, y_train, y_test = train_test_split(dfX, y, test_size=0.25, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "importances = clf.feature_importances_\n",
        "fi = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
        "print(\"Feature importances (Decision Tree, criterion='gini'):\")\n",
        "print(fi)\n",
        "print('\\nTrain accuracy:', clf.score(X_train, y_train))\n",
        "print('Test accuracy:', clf.score(X_test, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "842748a5",
      "metadata": {
        "id": "842748a5"
      },
      "source": [
        "### Question 5: **What is a Support Vector Machine (SVM)?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6de3568",
      "metadata": {
        "id": "a6de3568"
      },
      "source": [
        "\n",
        "A Support Vector Machine (SVM) is a supervised learning algorithm for classification (and regression) that finds the hyperplane that best separates classes by maximizing the margin — the distance between the hyperplane and the nearest points of each class (support vectors). SVMs can work in high-dimensional spaces and are effective when classes are separable with a clear margin. For non-linearly separable data, SVMs use kernel functions to project data into higher-dimensional spaces.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9e354d6",
      "metadata": {
        "id": "d9e354d6"
      },
      "source": [
        "### Question 6:**What is the Kernel Trick in SVM?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cdd5494",
      "metadata": {
        "id": "8cdd5494"
      },
      "source": [
        "\n",
        "The Kernel Trick allows SVMs to compute dot products in a high-dimensional (possibly infinite-dimensional) feature space without explicitly mapping data to that space. Instead, a kernel function K(x, x') computes \\(\\phi(x)\\cdot\\phi(x')\\) directly. Common kernels: linear, polynomial, RBF (Gaussian), sigmoid. This enables SVMs to learn non-linear decision boundaries efficiently.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5624085",
      "metadata": {
        "id": "c5624085"
      },
      "source": [
        "### Question 7: **Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a9d2c7bd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9d2c7bd",
        "outputId": "936f3878-aac1-45e5-83f8-7de83957c20e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear SVM accuracy: 0.9556\n",
            "RBF SVM accuracy:    0.9778\n"
          ]
        }
      ],
      "source": [
        "# Q7: Compare SVM with linear and RBF kernels on Wine dataset\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Standardize features for SVM\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "svm_linear.fit(X_train, y_train)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "pred_lin = svm_linear.predict(X_test)\n",
        "pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "acc_lin = accuracy_score(y_test, pred_lin)\n",
        "acc_rbf = accuracy_score(y_test, pred_rbf)\n",
        "\n",
        "print(f\"Linear SVM accuracy: {acc_lin:.4f}\")\n",
        "print(f\"RBF SVM accuracy:    {acc_rbf:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58a85e10",
      "metadata": {
        "id": "58a85e10"
      },
      "source": [
        "### Question 8: **What is the Naïve Bayes classifier, and why is it called \"Naïve\"?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f4b521d",
      "metadata": {
        "id": "8f4b521d"
      },
      "source": [
        "\n",
        "Naïve Bayes is a family of probabilistic classifiers based on Bayes' theorem:  \n",
        "\\(P(y|x) \\propto P(y) \\prod_i P(x_i | y)\\).  \n",
        "It's called *naïve* because it assumes conditional independence between features given the class label (i.e., features are independent of each other within each class). Despite this strong (and often unrealistic) assumption, Naïve Bayes works well in many practical tasks like text classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "872df7e2",
      "metadata": {
        "id": "872df7e2"
      },
      "source": [
        "### Question 9: **Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c5318dc",
      "metadata": {
        "id": "4c5318dc"
      },
      "source": [
        "\n",
        "- **Gaussian Naïve Bayes (GaussianNB):** Assumes continuous features follow a Gaussian (normal) distribution. Used for continuous-valued inputs (e.g., sensor data).  \n",
        "- **Multinomial Naïve Bayes:** Designed for count data (e.g., word counts in documents). Models P(x_i|y) with a multinomial distribution; works well with TF or raw term counts.  \n",
        "- **Bernoulli Naïve Bayes:** Designed for binary features (e.g., word presence/absence). Models each feature as a Bernoulli (0/1) variable.  \n",
        "**Which to choose:** depends on data type: continuous -> Gaussian, counts -> Multinomial, binary indicators -> Bernoulli.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9799d5da",
      "metadata": {
        "id": "9799d5da"
      },
      "source": [
        "### Question 10: **Breast Cancer Dataset - Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c18bec9a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c18bec9a",
        "outputId": "feb71062-c443-4d36-a5ad-7e78c2eb9c4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GaussianNB accuracy on Breast Cancer test set: 0.9371\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.94      0.89      0.91        53\n",
            "      benign       0.94      0.97      0.95        90\n",
            "\n",
            "    accuracy                           0.94       143\n",
            "   macro avg       0.94      0.93      0.93       143\n",
            "weighted avg       0.94      0.94      0.94       143\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Q10: Train Gaussian Naive Bayes on Breast Cancer dataset and evaluate\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Standardize (GaussianNB doesn't require scaling but it can help interpretation)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "pred = gnb.predict(X_test)\n",
        "acc = accuracy_score(y_test, pred)\n",
        "print(f\"GaussianNB accuracy on Breast Cancer test set: {acc:.4f}\\n\")\n",
        "print('Classification report:\\n', classification_report(y_test, pred, target_names=data.target_names))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}